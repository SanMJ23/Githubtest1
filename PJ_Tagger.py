#!/usr/bin/env python3
"""
PJ_Tagger.py â€” Bulk (Un)Tagger for AWS Resources via Resource Groups Tagging API

Key features:
- Tag and untag in bulk from a CSV (adds from normal columns, removes from `Untag:<Key>` columns)
- Dry run mode, batching, grouping identical actions, retries with jitter backoff
- Optional ARN existence validation, optional concurrency across groups
- Clear CSV result log and CI-friendly exit codes

Author: Regenerated by M365 Copilot for Praveen Jayasekaran
"""
import argparse
import csv
import datetime as dt
import os
import re
import sys
import time
import logging
from typing import Dict, List, Tuple, Iterable, Any
from concurrent.futures import ThreadPoolExecutor, as_completed

import boto3
from botocore.config import Config
from botocore.exceptions import ClientError, BotoCoreError

# -------------------- Defaults & Limits --------------------
DEFAULT_INPUT = 'tags.csv'
DEFAULT_BATCH_SIZE = 20  # conservative; adjust via --batch-size
DEFAULT_CONCURRENCY = 1  # safe default; can raise to 4-8 if needed
MAX_TAG_KEY_CHARS = 128
MAX_TAG_VALUE_CHARS = 256
UNTAG_PREFIX_DEFAULT = "Untag:"

# Relaxed AWS-safe chars for KEYS (values are generally free-form)
TAG_KEY_PATTERN = re.compile(r'^[\w\s+\-=._:/@]+$')

# Permissive ARN quick-check; rely on API validation when not skipping
ARN_QUICK = re.compile(r"^arn:[^:]+:[^:]*:[^:]*:[^:]*:.+")

# -------------------- Logging --------------------
LOG = logging.getLogger("bulk_tagger")

def setup_logging(verbose: bool):
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s %(levelname)-8s %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )

# -------------------- Utilities --------------------
def chunked(seq: List[str], size: int) -> Iterable[List[str]]:
    for i in range(0, len(seq), size):
        yield seq[i:i+size]

def arn_sanity_check(arn: str) -> bool:
    return bool(ARN_QUICK.match(arn))

def validate_tags_shape(tags: Dict[str, str]) -> Tuple[bool, str]:
    for k, v in tags.items():
        if not k:
            return False, "Empty tag key"
        if len(k) > MAX_TAG_KEY_CHARS:
            return False, f"Tag key too long (> {MAX_TAG_KEY_CHARS} chars): {k}"
        if v is None:
            return False, f"Tag value None for key {k}"
        if len(v) > MAX_TAG_VALUE_CHARS:
            return False, f"Tag value too long (> {MAX_TAG_VALUE_CHARS} chars) for key {k}"
        if k.lower().startswith("aws:"):
            return False, "Tag keys may not start with the reserved prefix 'aws:'"
        if not TAG_KEY_PATTERN.match(k):
            return False, f"Tag key contains unusual characters: {k}"
    return True, ""

# -------------------- Boto session --------------------
def build_clients(profile: str, region: str, max_attempts: int):
    """Build clients using ambient credentials. No assume-role logic here."""
    config = Config(
        retries={"max_attempts": max(2, max_attempts), "mode": "standard"},
        user_agent_extra="bulk-tagger/2.0"
    )
    sess = boto3.Session(profile_name=profile, region_name=region or None)
    tag_client = sess.client("resourcegroupstaggingapi", config=config)
    sts_client = sess.client("sts", config=config)
    return sess, tag_client, sts_client

# -------------------- CSV I/O --------------------
def read_csv(file_path: str, resource_col: str) -> Tuple[List[str], List[Dict[str, str]]]:
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Input file not found: {file_path}")
    with open(file_path, "r", newline="", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        if resource_col not in reader.fieldnames:
            raise ValueError(f"CSV must include a '{resource_col}' column. Found: {reader.fieldnames}")
        rows = []
        for row in reader:
            clean = {(k.strip() if isinstance(k, str) else k): (v.strip() if isinstance(v, str) else v)
                     for k, v in row.items()}
            rows.append(clean)
        return reader.fieldnames, rows

def write_log(log_file: str, rows: List[Dict[str, str]]):
    with open(log_file, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=["ResourceARN", "Action", "Status", "Message"])
        writer.writeheader()
        writer.writerows(rows)

# -------------------- Validation via Tagging API --------------------
def validate_arn_exists(tag_client, arn: str) -> Tuple[bool, str]:
    """Best-effort: ask the Tagging API if it can see the ARN.
    AccessDenied is treated as 'assume exists' because permissions vary.
    """
    try:
        tag_client.get_resources(ResourceARNList=[arn])
        return True, "OK"
    except ClientError as e:
        msg = str(e)
        if "AccessDenied" in msg or "Forbidden" in msg:
            return True, "AccessDenied (assuming exists)"
        return False, f"Invalid or inaccessible ({msg})"
    except Exception as e:
        return False, f"Error validating ARN ({e})"

# -------------------- Tagging Core --------------------
def detect_transient(messages: List[str]) -> bool:
    tokens = (
        "Throttl", "TooManyRequests", "ThrottlingException", "RequestLimitExceeded",
        "InternalService", "ServiceUnavailable", "Timeout", "RequestTimeout", "GatewayTimeout"
    )
    text = " | ".join(m or "" for m in messages)
    return any(tok in text for tok in tokens)

def exponential_backoff(attempt: int, base: float = 0.5, cap: float = 8.0):
    import random
    sleep = min(cap, base * (2 ** attempt))
    # full jitter in [0.5x, 1.5x]
    time.sleep(sleep * (0.5 + random.random()))

def tag_batch(tag_client, arns: List[str], tags: Dict[str, str]) -> Tuple[List[str], Dict[str, str]]:
    """Returns (succeeded_arns, failed_dict[arn] -> message)"""
    succeeded: List[str] = []
    failed: Dict[str, str] = {}
    try:
        resp = tag_client.tag_resources(ResourceARNList=arns, Tags=tags)
        failures = resp.get("FailedResourcesMap", {}) or {}
        if not failures:
            succeeded.extend(arns)
            return succeeded, failed
        failed_arns = set()
        for arn, details in failures.items():
            code = details.get("ErrorCode", "Unknown")
            msg = details.get("ErrorMessage", "Unknown error")
            failed[arn] = f"{code}: {msg}"
            failed_arns.add(arn)
        for arn in arns:
            if arn not in failed_arns:
                succeeded.append(arn)
    except (ClientError, BotoCoreError) as e:
        for arn in arns:
            failed[arn] = f"ClientError: {e}"
    except Exception as e:
        for arn in arns:
            failed[arn] = f"Exception: {e}"
    return succeeded, failed

def untag_batch(tag_client, arns: List[str], tag_keys: List[str]) -> Tuple[List[str], Dict[str, str]]:
    succeeded: List[str] = []
    failed: Dict[str, str] = {}
    try:
        resp = tag_client.untag_resources(ResourceARNList=arns, TagKeys=tag_keys)
        failures = resp.get("FailedResourcesMap", {}) or {}
        if not failures:
            succeeded.extend(arns)
            return succeeded, failed
        failed_arns = set()
        for arn, details in failures.items():
            code = details.get("ErrorCode", "Unknown")
            msg = details.get("ErrorMessage", "Unknown error")
            failed[arn] = f"{code}: {msg}"
            failed_arns.add(arn)
        for arn in arns:
            if arn not in failed_arns:
                succeeded.append(arn)
    except (ClientError, BotoCoreError) as e:
        for arn in arns:
            failed[arn] = f"ClientError: {e}"
    except Exception as e:
        for arn in arns:
            failed[arn] = f"Exception: {e}"
    return succeeded, failed

# -------------------- Main Flow --------------------
def process_file(
    input_file: str,
    resource_col: str,
    dry_run: bool,
    batch_size: int,
    skip_validate: bool,
    profile: str,
    region: str,
    max_attempts: int,
    concurrency: int,
    untag_prefix: str,
) -> Tuple[str, Dict[str, Any]]:
    _, rows = read_csv(input_file, resource_col)
    _, tag_client, sts_client = build_clients(profile, region, max_attempts)

    # Identity echo
    try:
        ident = sts_client.get_caller_identity()
        LOG.info("Running as AWS Account: %s, ARN: %s", ident.get("Account"), ident.get("Arn"))
    except Exception:
        LOG.warning("Could not fetch AWS identity info (check credentials).")

    # Prepare log
    log_rows: List[Dict[str, str]] = []
    ts = dt.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = f"tag_results_{ts}.csv"

    # Group by identical add/remove sets for efficiency
    # key: (tuple(sorted(add_tags.items())), tuple(sorted(remove_keys))) -> [arn...]
    groups: Dict[Tuple[Tuple[Tuple[str, str], ...], Tuple[str, ...]], List[str]] = {}

    # Preprocess CSV rows
    UNTAG_TRUE = {"1", "true", "yes", "y"}

    for r in rows:
        arn = r.get(resource_col)
        if not arn:
            LOG.warning("Skipping row without %s", resource_col)
            log_rows.append({"ResourceARN": "", "Action": "N/A", "Status": "Skipped", "Message": f"Missing {resource_col}"})
            continue
        if not arn_sanity_check(arn):
            LOG.warning("ARN failed sanity check: %s", arn)
            log_rows.append({"ResourceARN": arn, "Action": "N/A", "Status": "Invalid", "Message": "Failed ARN sanity check"})
            continue

        # Extract add-tags (non-empty, excluding the resource column and UNTAG columns)
        add_tags: Dict[str, str] = {
            k: v for k, v in r.items()
            if k != resource_col and v not in (None, "") and not k.startswith(untag_prefix)
        }

        # Extract remove-keys (columns like 'Untag:CostCenter' with truthy values)
        remove_keys: List[str] = []
        for k, v in r.items():
            if k.startswith(untag_prefix):
                key_to_remove = k[len(untag_prefix):].strip()
                if key_to_remove and (v or "").strip().lower() in UNTAG_TRUE:
                    remove_keys.append(key_to_remove)

        if not add_tags and not remove_keys:
            LOG.info("No tag actions for %s, skipping.", arn)
            log_rows.append({"ResourceARN": arn, "Action": "None", "Status": "Skipped", "Message": "No tags to add or remove"})
            continue

        ok, why = validate_tags_shape(add_tags) if add_tags else (True, "")
        if not ok:
            LOG.warning("Invalid tags for %s: %s", arn, why)
            log_rows.append({"ResourceARN": arn, "Action": "Add", "Status": "InvalidTags", "Message": why})
            continue

        # Optional best-effort validation
        if not skip_validate:
            valid, vmsg = validate_arn_exists(tag_client, arn)
            if not valid:
                LOG.warning("Invalid or inaccessible ARN: %s (%s)", arn, vmsg)
                log_rows.append({"ResourceARN": arn, "Action": "N/A", "Status": "Invalid", "Message": vmsg})
                continue

        key_tuple = (tuple(sorted(add_tags.items())), tuple(sorted(remove_keys)))
        groups.setdefault(key_tuple, []).append(arn)

    # Early exit if dry-run
    if dry_run:
        for (add_tuple, remove_tuple), arns in groups.items():
            add_dict = dict(add_tuple)
            LOG.info("[DRY RUN] Would tag %d resources: add=%s remove=%s", len(arns), add_dict, list(remove_tuple))
            for arn in arns:
                if add_tuple:
                    log_rows.append({"ResourceARN": arn, "Action": "Add", "Status": "DryRun", "Message": str(add_dict)})
                if remove_tuple:
                    log_rows.append({"ResourceARN": arn, "Action": "Remove", "Status": "DryRun", "Message": str(list(remove_tuple))})
        write_log(log_file, log_rows)
        summary = {
            "success": 0, "failed": 0,
            "skipped": sum(1 for r in log_rows if r["Status"] in ("Skipped", "Invalid", "InvalidTags")),
            "dryrun_actions": sum(1 for r in log_rows if r["Status"] == "DryRun"),
            "groups": len(groups), "resources": sum(len(v) for v in groups.values()),
        }
        return log_file, summary

    # Live mode helpers
    def process_group(group_key, arns: List[str]) -> Tuple[List[Dict[str, str]], Dict[str, int]]:
        add_tuple, remove_tuple = group_key
        add_tags_local = dict(add_tuple)
        remove_keys_local = list(remove_tuple)
        local_logs: List[Dict[str, str]] = []
        counts = {"add_success": 0, "add_failed": 0, "rm_success": 0, "rm_failed": 0}

        # Add tags first
        if add_tags_local:
            for batch_idx, batch in enumerate(chunked(arns, batch_size), start=1):
                attempt = 0
                while True:
                    succeeded, failed = tag_batch(tag_client, batch, add_tags_local)
                    msgs = list(failed.values())
                    if detect_transient(msgs) and attempt < 5:
                        LOG.warning("Transient failure on ADD batch %d (attempt %d). Retrying...", batch_idx, attempt + 1)
                        exponential_backoff(attempt)
                        attempt += 1
                        continue
                    # Record results
                    for arn in succeeded:
                        local_logs.append({"ResourceARN": arn, "Action": "Add", "Status": "Success", "Message": str(add_tags_local)})
                    for arn, msg in failed.items():
                        local_logs.append({"ResourceARN": arn, "Action": "Add", "Status": "Failed", "Message": msg})
                    counts["add_success"] += len(succeeded)
                    counts["add_failed"] += len(failed)
                    break

        # Then remove tags (if any)
        if remove_keys_local:
            for batch_idx, batch in enumerate(chunked(arns, batch_size), start=1):
                attempt = 0
                while True:
                    succeeded, failed = untag_batch(tag_client, batch, remove_keys_local)
                    msgs = list(failed.values())
                    if detect_transient(msgs) and attempt < 5:
                        LOG.warning("Transient failure on REMOVE batch %d (attempt %d). Retrying...", batch_idx, attempt + 1)
                        exponential_backoff(attempt)
                        attempt += 1
                        continue
                    # Record results
                    for arn in succeeded:
                        local_logs.append({"ResourceARN": arn, "Action": "Remove", "Status": "Success", "Message": str(remove_keys_local)})
                    for arn, msg in failed.items():
                        local_logs.append({"ResourceARN": arn, "Action": "Remove", "Status": "Failed", "Message": msg})
                    counts["rm_success"] += len(succeeded)
                    counts["rm_failed"] += len(failed)
                    break
        return local_logs, counts

    total_add_success = total_add_failed = 0
    total_rm_success = total_rm_failed = 0
    total_skipped = sum(1 for r in log_rows if r["Status"] in ("Skipped", "Invalid", "InvalidTags"))

    # Concurrency across groups
    if concurrency <= 1 or len(groups) <= 1:
        for gk, arns in groups.items():
            LOG.info("Processing group: %d resources | add=%s remove=%s", len(arns), dict(gk[0]), list(gk[1]))
            local_logs, counts = process_group(gk, arns)
            log_rows.extend(local_logs)
            total_add_success += counts["add_success"]
            total_add_failed  += counts["add_failed"]
            total_rm_success  += counts["rm_success"]
            total_rm_failed   += counts["rm_failed"]
    else:
        with ThreadPoolExecutor(max_workers=concurrency) as ex:
            futures = {ex.submit(process_group, gk, arns): gk for gk, arns in groups.items()}
            for fut in as_completed(futures):
                local_logs, counts = fut.result()
                log_rows.extend(local_logs)
                total_add_success += counts["add_success"]
                total_add_failed  += counts["add_failed"]
                total_rm_success  += counts["rm_success"]
                total_rm_failed   += counts["rm_failed"]

    write_log(log_file, log_rows)
    LOG.info("Log saved to: %s", log_file)
    LOG.info(
        "Summary: Add: Success=%d Failed=%d | Remove: Success=%d Failed=%d | Skipped/Invalid=%d",
        total_add_success, total_add_failed, total_rm_success, total_rm_failed, total_skipped
    )

    summary = {
        "add_success": total_add_success,
        "add_failed": total_add_failed,
        "remove_success": total_rm_success,
        "remove_failed": total_rm_failed,
        "skipped": total_skipped,
        "groups": len(groups),
        "resources": sum(len(v) for v in groups.values()),
        "log_file": log_file,
    }
    return log_file, summary


def main():
    parser = argparse.ArgumentParser(description="Tag/Untag AWS resources from a CSV file via the Resource Groups Tagging API.")
    parser.add_argument("--file", type=str, default=DEFAULT_INPUT, help="Path to CSV file (default: tags.csv)")
    parser.add_argument("--dry-run", action="store_true", help="Preview actions without applying changes")
    parser.add_argument("--batch-size", type=int, default=DEFAULT_BATCH_SIZE, help=f"Batch size per API call (default: {DEFAULT_BATCH_SIZE})")
    parser.add_argument("--skip-validate", action="store_true", help="Skip ARN existence validation (faster, less safe)")
    parser.add_argument("--resource-col", type=str, default="ResourceARN", help="CSV column containing the ARN (default: ResourceARN)")
    parser.add_argument("--profile", type=str, default=None, help="AWS profile name (uses env/IMDS if omitted)")
    parser.add_argument("--region", type=str, default=None, help="AWS region (optional)")
    parser.add_argument("--max-attempts", type=int, default=5, help="Botocore retry attempts (default: 5)")
    parser.add_argument("--concurrency", type=int, default=DEFAULT_CONCURRENCY, help=f"Number of groups to process concurrently (default: {DEFAULT_CONCURRENCY})")
    parser.add_argument("--untag-prefix", type=str, default=UNTAG_PREFIX_DEFAULT, help=f"Prefix for untag columns (default: '{UNTAG_PREFIX_DEFAULT}')")
    parser.add_argument("--verbose", action="store_true", help="Verbose logging")

    args = parser.parse_args()
    setup_logging(args.verbose)

    try:
        log_file, summary = process_file(
            input_file=args.file,
            resource_col=args.resource_col,
            dry_run=args.dry_run,
            batch_size=max(1, args.batch_size),
            skip_validate=args.skip_validate,
            profile=args.profile,
            region=args.region,
            max_attempts=max(2, args.max_attempts),
            concurrency=max(1, args.concurrency),
            untag_prefix=args.untag_prefix,
        )
        LOG.info("Completed. See %s", log_file)
        # Exit code: 0 if no failures, 4 otherwise (so CI can gate)
        failures = summary["add_failed"] + summary["remove_failed"]
        sys.exit(0 if failures == 0 else 4)
    except FileNotFoundError as e:
        LOG.error(str(e))
        sys.exit(2)
    except ValueError as e:
        LOG.error(str(e))
        sys.exit(3)
    except Exception as e:
        LOG.exception("Unhandled error: %s", e)
        sys.exit(1)

if __name__ == "__main__":
    main()